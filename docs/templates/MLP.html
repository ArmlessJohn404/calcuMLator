<template>
  <p>
      The <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron"><b>Multi
      -Layer Perceptron</b></a> is not structurally different from a
      <a href="https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks">
      Deep Neural Network</a>, but there's some differences in the parameters
      initialization, in the backpropagation algorithms and in some other tricks.
      In this case, there's a single hidden layer of 10 units between the input
      and output.
  </p>
  <p>
    It's intresting to see that the MLP managed to discover a curve similar to
    the real multiplication, even though the input are just the two numbers with
    no kernels. This happens because of the hidden layer that is able to extract
    more information about the problem.
  <p>
  <img src="images/add_MLP.png"/>
  <img src="images/sub_MLP.png"/>
  <img src="images/mul_MLP.png"/>
  <img src="images/div_MLP.png"/>
</template>
